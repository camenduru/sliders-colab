{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/sliders-colab/blob/main/sliders/sliders_dark_weather_gradio_colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/sliders\n",
        "!pip install -q bitsandbytes==0.41.1 dadaptation==3.1 diffusers==0.20.2 lion_pytorch==0.1.2 lpips==0.1.4 prodigyopt==1.0 accelerate==0.22.0 gradio==3.50.2\n",
        "!pip install -q https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/age.pt -d /content/sliders -o age.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/cartoon_style.pt -d /content/sliders -o cartoon_style.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/chubby.pt -d /content/sliders -o chubby.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/clay_style.pt -d /content/sliders -o clay_style.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/cluttered_room.pt -d /content/sliders -o cluttered_room.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/curlyhair.pt -d /content/sliders -o curlyhair.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/dark_weather.pt -d /content/sliders -o dark_weather.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/eyebrow.pt -d /content/sliders -o eyebrow.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/eyesize.pt -d /content/sliders -o eyesize.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/festive.pt -d /content/sliders -o festive.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/fix_hands.pt -d /content/sliders -o fix_hands.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/long_hair.pt -d /content/sliders -o long_hair.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/muscular.pt -d /content/sliders -o muscular.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/pixar_style.pt -d /content/sliders -o pixar_style.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/professional.pt -d /content/sliders -o professional.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/repair_slider.pt -d /content/sliders -o repair_slider.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/sculpture_style.pt -d /content/sliders -o sculpture_style.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/smiling.pt -d /content/sliders -o smiling.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/stylegan_latent1.pt -d /content/sliders -o stylegan_latent1.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/stylegan_latent2.pt -d /content/sliders -o stylegan_latent2.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/suprised_look.pt -d /content/sliders -o suprised_look.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/tropical_weather.pt -d /content/sliders -o tropical_weather.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/xl_sliders/resolve/main/winter_weather.pt -d /content/sliders -o winter_weather.pt\n",
        "\n",
        "%cd /content/sliders\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import os, json, random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob, re\n",
        "\n",
        "from safetensors.torch import load_file\n",
        "import matplotlib.image as mpimg\n",
        "import copy\n",
        "import gc\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "import diffusers\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel, LMSDiscreteScheduler\n",
        "from diffusers.loaders import AttnProcsLayers\n",
        "from diffusers.models.attention_processor import LoRAAttnProcessor, AttentionProcessor\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from trainscripts.textsliders.lora import LoRANetwork, DEFAULT_TARGET_REPLACE, UNET_TARGET_REPLACE_MODULE_CONV\n",
        "from diffusers.pipelines.stable_diffusion_xl import StableDiffusionXLPipelineOutput\n",
        "\n",
        "import inspect\n",
        "import os\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "from diffusers.pipelines import StableDiffusionXLPipeline\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTokenizer\n",
        "    \n",
        "@torch.no_grad()\n",
        "def call(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]] = None,\n",
        "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "        height: Optional[int] = None,\n",
        "        width: Optional[int] = None,\n",
        "        num_inference_steps: int = 50,\n",
        "        denoising_end: Optional[float] = None,\n",
        "        guidance_scale: float = 5.0,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "        num_images_per_prompt: Optional[int] = 1,\n",
        "        eta: float = 0.0,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "        callback_steps: int = 1,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        guidance_rescale: float = 0.0,\n",
        "        original_size: Optional[Tuple[int, int]] = None,\n",
        "        crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
        "        target_size: Optional[Tuple[int, int]] = None,\n",
        "        negative_original_size: Optional[Tuple[int, int]] = None,\n",
        "        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
        "        negative_target_size: Optional[Tuple[int, int]] = None,\n",
        "    \n",
        "        network=None, \n",
        "        start_noise=None,\n",
        "        scale=None,\n",
        "        unet=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Function invoked when calling the pipeline for generation.\n",
        "\n",
        "        Args:\n",
        "            prompt (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n",
        "                instead.\n",
        "            prompt_2 (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n",
        "                used in both text-encoders\n",
        "            height (`int`, *optional*, defaults to unet.config.sample_size * self.vae_scale_factor):\n",
        "                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n",
        "                Anything below 512 pixels won't work well for\n",
        "                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n",
        "                and checkpoints that are not specifically fine-tuned on low resolutions.\n",
        "            width (`int`, *optional*, defaults to unet.config.sample_size * self.vae_scale_factor):\n",
        "                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n",
        "                Anything below 512 pixels won't work well for\n",
        "                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n",
        "                and checkpoints that are not specifically fine-tuned on low resolutions.\n",
        "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
        "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
        "                expense of slower inference.\n",
        "            denoising_end (`float`, *optional*):\n",
        "                When specified, determines the fraction (between 0.0 and 1.0) of the total denoising process to be\n",
        "                completed before it is intentionally prematurely terminated. As a result, the returned sample will\n",
        "                still retain a substantial amount of noise as determined by the discrete timesteps selected by the\n",
        "                scheduler. The denoising_end parameter should ideally be utilized when this pipeline forms a part of a\n",
        "                \"Mixture of Denoisers\" multi-pipeline setup, as elaborated in [**Refining the Image\n",
        "                Output**](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)\n",
        "            guidance_scale (`float`, *optional*, defaults to 5.0):\n",
        "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
        "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
        "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
        "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
        "                usually at the expense of lower image quality.\n",
        "            negative_prompt (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
        "                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
        "                less than `1`).\n",
        "            negative_prompt_2 (`str` or `List[str]`, *optional*):\n",
        "                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and\n",
        "                `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders\n",
        "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
        "                The number of images to generate per prompt.\n",
        "            eta (`float`, *optional*, defaults to 0.0):\n",
        "                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
        "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
        "            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
        "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
        "                to make generation deterministic.\n",
        "            latents (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
        "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
        "                tensor will ge generated by sampling using the supplied random `generator`.\n",
        "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
        "                provided, text embeddings will be generated from `prompt` input argument.\n",
        "            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
        "                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
        "                argument.\n",
        "            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.\n",
        "                If not provided, pooled text embeddings will be generated from `prompt` input argument.\n",
        "            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
        "                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`\n",
        "                input argument.\n",
        "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
        "                The output format of the generate image. Choose between\n",
        "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
        "            return_dict (`bool`, *optional*, defaults to `True`):\n",
        "                Whether or not to return a [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] instead\n",
        "                of a plain tuple.\n",
        "            callback (`Callable`, *optional*):\n",
        "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
        "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
        "            callback_steps (`int`, *optional*, defaults to 1):\n",
        "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
        "                called at every step.\n",
        "            cross_attention_kwargs (`dict`, *optional*):\n",
        "                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
        "                `self.processor` in\n",
        "                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/sliders/src/diffusers/models/attention_processor.py).\n",
        "            guidance_rescale (`float`, *optional*, defaults to 0.7):\n",
        "                Guidance rescale factor proposed by [Common Diffusion Noise Schedules and Sample Steps are\n",
        "                Flawed](https://arxiv.org/pdf/2305.08891.pdf) `guidance_scale` is defined as `φ` in equation 16. of\n",
        "                [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).\n",
        "                Guidance rescale factor should fix overexposure when using zero terminal SNR.\n",
        "            original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n",
        "                If `original_size` is not the same as `target_size` the image will appear to be down- or upsampled.\n",
        "                `original_size` defaults to `(width, height)` if not specified. Part of SDXL's micro-conditioning as\n",
        "                explained in section 2.2 of\n",
        "                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n",
        "            crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n",
        "                `crops_coords_top_left` can be used to generate an image that appears to be \"cropped\" from the position\n",
        "                `crops_coords_top_left` downwards. Favorable, well-centered images are usually achieved by setting\n",
        "                `crops_coords_top_left` to (0, 0). Part of SDXL's micro-conditioning as explained in section 2.2 of\n",
        "                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n",
        "            target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n",
        "                For most cases, `target_size` should be set to the desired height and width of the generated image. If\n",
        "                not specified it will default to `(width, height)`. Part of SDXL's micro-conditioning as explained in\n",
        "                section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n",
        "            negative_original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n",
        "                To negatively condition the generation process based on a specific image resolution. Part of SDXL's\n",
        "                micro-conditioning as explained in section 2.2 of\n",
        "                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n",
        "                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n",
        "            negative_crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n",
        "                To negatively condition the generation process based on a specific crop coordinates. Part of SDXL's\n",
        "                micro-conditioning as explained in section 2.2 of\n",
        "                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n",
        "                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n",
        "            negative_target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n",
        "                To negatively condition the generation process based on a target image resolution. It should be as same\n",
        "                as the `target_size` for most cases. Part of SDXL's micro-conditioning as explained in section 2.2 of\n",
        "                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n",
        "                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n",
        "\n",
        "        Examples:\n",
        "\n",
        "        Returns:\n",
        "            [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] or `tuple`:\n",
        "            [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] if `return_dict` is True, otherwise a\n",
        "            `tuple`. When returning a tuple, the first element is a list with the generated images.\n",
        "        \"\"\"\n",
        "        # 0. Default height and width to unet\n",
        "        height = height or self.default_sample_size * self.vae_scale_factor\n",
        "        width = width or self.default_sample_size * self.vae_scale_factor\n",
        "\n",
        "        original_size = original_size or (height, width)\n",
        "        target_size = target_size or (height, width)\n",
        "\n",
        "        # 1. Check inputs. Raise error if not correct\n",
        "        self.check_inputs(\n",
        "            prompt,\n",
        "            prompt_2,\n",
        "            height,\n",
        "            width,\n",
        "            callback_steps,\n",
        "            negative_prompt,\n",
        "            negative_prompt_2,\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds,\n",
        "        )\n",
        "\n",
        "        # 2. Define call parameters\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        device = self._execution_device\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "\n",
        "        # 3. Encode input prompt\n",
        "        text_encoder_lora_scale = (\n",
        "            cross_attention_kwargs.get(\"scale\", None) if cross_attention_kwargs is not None else None\n",
        "        )\n",
        "        (\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds,\n",
        "        ) = self.encode_prompt(\n",
        "            prompt=prompt,\n",
        "            prompt_2=prompt_2,\n",
        "            device=device,\n",
        "            num_images_per_prompt=num_images_per_prompt,\n",
        "            do_classifier_free_guidance=do_classifier_free_guidance,\n",
        "            negative_prompt=negative_prompt,\n",
        "            negative_prompt_2=negative_prompt_2,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
        "            lora_scale=text_encoder_lora_scale,\n",
        "        )\n",
        "\n",
        "        # 4. Prepare timesteps\n",
        "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "\n",
        "        timesteps = self.scheduler.timesteps\n",
        "\n",
        "        # 5. Prepare latent variables\n",
        "        num_channels_latents = unet.config.in_channels\n",
        "        latents = self.prepare_latents(\n",
        "            batch_size * num_images_per_prompt,\n",
        "            num_channels_latents,\n",
        "            height,\n",
        "            width,\n",
        "            prompt_embeds.dtype,\n",
        "            device,\n",
        "            generator,\n",
        "            latents,\n",
        "        )\n",
        "\n",
        "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
        "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "        # 7. Prepare added time ids & embeddings\n",
        "        add_text_embeds = pooled_prompt_embeds\n",
        "        add_time_ids = self._get_add_time_ids(\n",
        "            original_size, crops_coords_top_left, target_size, dtype=prompt_embeds.dtype\n",
        "        )\n",
        "        if negative_original_size is not None and negative_target_size is not None:\n",
        "            negative_add_time_ids = self._get_add_time_ids(\n",
        "                negative_original_size,\n",
        "                negative_crops_coords_top_left,\n",
        "                negative_target_size,\n",
        "                dtype=prompt_embeds.dtype,\n",
        "            )\n",
        "        else:\n",
        "            negative_add_time_ids = add_time_ids\n",
        "\n",
        "        if do_classifier_free_guidance:\n",
        "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
        "            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
        "            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
        "\n",
        "        prompt_embeds = prompt_embeds.to(device)\n",
        "        add_text_embeds = add_text_embeds.to(device)\n",
        "        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
        "\n",
        "        # 8. Denoising loop\n",
        "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
        "\n",
        "        # 7.1 Apply denoising_end\n",
        "        if denoising_end is not None and isinstance(denoising_end, float) and denoising_end > 0 and denoising_end < 1:\n",
        "            discrete_timestep_cutoff = int(\n",
        "                round(\n",
        "                    self.scheduler.config.num_train_timesteps\n",
        "                    - (denoising_end * self.scheduler.config.num_train_timesteps)\n",
        "                )\n",
        "            )\n",
        "            num_inference_steps = len(list(filter(lambda ts: ts >= discrete_timestep_cutoff, timesteps)))\n",
        "            timesteps = timesteps[:num_inference_steps]\n",
        "        latents = latents.to(unet.dtype)\n",
        "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "                if t>start_noise:\n",
        "                    network.set_lora_slider(scale=0)\n",
        "                else:\n",
        "                    network.set_lora_slider(scale=scale)\n",
        "                # expand the latents if we are doing classifier free guidance\n",
        "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "                # predict the noise residual\n",
        "                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
        "                with network:\n",
        "                    noise_pred = unet(\n",
        "                        latent_model_input,\n",
        "                        t,\n",
        "                        encoder_hidden_states=prompt_embeds,\n",
        "                        cross_attention_kwargs=cross_attention_kwargs,\n",
        "                        added_cond_kwargs=added_cond_kwargs,\n",
        "                        return_dict=False,\n",
        "                    )[0]\n",
        "\n",
        "                # perform guidance\n",
        "                if do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                if do_classifier_free_guidance and guidance_rescale > 0.0:\n",
        "                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n",
        "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=guidance_rescale)\n",
        "\n",
        "                # compute the previous noisy sample x_t -> x_t-1\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
        "\n",
        "                # call the callback, if provided\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "                    if callback is not None and i % callback_steps == 0:\n",
        "                        callback(i, t, latents)\n",
        "\n",
        "        if not output_type == \"latent\":\n",
        "            # make sure the VAE is in float32 mode, as it overflows in float16\n",
        "            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n",
        "\n",
        "            if needs_upcasting:\n",
        "                self.upcast_vae()\n",
        "                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n",
        "\n",
        "            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
        "\n",
        "            # cast back to fp16 if needed\n",
        "            if needs_upcasting:\n",
        "                self.vae.to(dtype=torch.float16)\n",
        "        else:\n",
        "            image = latents\n",
        "\n",
        "        if not output_type == \"latent\":\n",
        "            # apply watermark if available\n",
        "            if self.watermark is not None:\n",
        "                image = self.watermark.apply_watermark(image)\n",
        "\n",
        "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
        "\n",
        "        # Offload all models\n",
        "#         self.maybe_free_model_hooks()\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image,)\n",
        "\n",
        "        return StableDiffusionXLPipelineOutput(images=image)\n",
        "\n",
        "global pipe\n",
        "global unet\n",
        "global network\n",
        "\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained('stabilityai/stable-diffusion-xl-base-1.0', torch_dtype=torch.float16).to('cuda')\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "unet = pipe.unet\n",
        "StableDiffusionXLPipeline.__call__ = call\n",
        "\n",
        "def generate(prompt, scale, num_inference_steps, guidance_scale):\n",
        "  lora_weight = 'dark_weather.pt'\n",
        "  start_noise = 700\n",
        "  num_images_per_prompt = 1\n",
        "  for _ in range(num_images_per_prompt):\n",
        "      seed = random.randint(0,2**15)\n",
        "      print(prompt, seed)\n",
        "      if 'full' in lora_weight:\n",
        "          train_method = 'full'\n",
        "      elif 'noxattn' in lora_weight:\n",
        "          train_method = 'noxattn'\n",
        "      else:\n",
        "          train_method = 'noxattn'\n",
        "      network_type = \"c3lier\"\n",
        "      if train_method == 'xattn':\n",
        "          network_type = 'lierla'\n",
        "      modules = DEFAULT_TARGET_REPLACE\n",
        "      if network_type == \"c3lier\":\n",
        "          modules += UNET_TARGET_REPLACE_MODULE_CONV\n",
        "      import os\n",
        "      model_name = lora_weight\n",
        "      name = os.path.basename(model_name)\n",
        "      rank = 4\n",
        "      alpha = 1\n",
        "      if 'rank4' in lora_weight:\n",
        "          rank = 4\n",
        "      if 'rank8' in lora_weight:\n",
        "          rank = 8\n",
        "      if 'alpha1' in lora_weight:\n",
        "          alpha = 1.0\n",
        "      network = LoRANetwork(unet, rank=rank, multiplier=1.0, alpha=alpha, train_method=train_method).to('cuda', dtype=torch.float16)\n",
        "      network.load_state_dict(torch.load(lora_weight))\n",
        "      generator = torch.manual_seed(seed)\n",
        "      image = pipe(prompt, num_images_per_prompt=1, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, generator=generator, network=network, start_noise=start_noise, scale=int(scale), unet=unet).images[0]\n",
        "      return image\n",
        "\n",
        "import gradio as gr\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "        textbox = gr.Textbox(value='Image of a person, realistic, 8k')\n",
        "        scale = gr.Slider(minimum=1, maximum=10, step=1, value=4)\n",
        "        num_inference_steps = gr.Slider(minimum=1, maximum=50, step=1, value=35, label=\"num_inference_steps\")\n",
        "        guidance_scale = gr.Slider(minimum=0, maximum=10, step=0.5, value=8, label=\"guidance_scale\")\n",
        "        btn = gr.Button(\"Run\")\n",
        "    with gr.Column():\n",
        "      image = gr.Image(label=\"output\", type=\"pil\")\n",
        "    btn.click(generate, inputs=[textbox, scale, num_inference_steps, guidance_scale], outputs=[image])\n",
        "  demo.queue().launch(debug=True, share=True, inline=False, show_error=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
